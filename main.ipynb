{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df7817d2",
   "metadata": {},
   "source": [
    "# RAG Based AI Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaea0e7e",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eb692c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# High-level helper with pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the Qwen 0.6B model\n",
    "pipe = pipeline(\"text-generation\", model=\"Qwen/Qwen3-0.6B\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d964e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input\n",
    "user_input = \"How's the weather today in Islamabad?\"\n",
    "\n",
    "# Generate response\n",
    "response = pipe(user_input, max_new_tokens=50, do_sample=True, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce9a8b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: How's the weather today in Islamabad?\n",
      "Bot: How's the weather today in Islamabad? The user is asking about the current weather in Islamabad. \n",
      "\n",
      "As a language model, I can't access real-time data. However, I can provide general information about the weather in Islamabad based on current knowledge. \n",
      "\n",
      "Islamabad is the capital of Pakistan\n"
     ]
    }
   ],
   "source": [
    "# Print result\n",
    "print(\"User:\", user_input)\n",
    "print(\"Bot:\", response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06853cec",
   "metadata": {},
   "source": [
    "## 2. Data Ingestion and Splitting Text\n",
    "\n",
    "üîπ **Goal:** Load documents into the chatbot system.  \n",
    "\n",
    "### Instructions:\n",
    "1. Place your document (e.g., `data.pdf`) inside a `docs/` folder.  \n",
    "\n",
    "2. Use **LangChain‚Äôs PyPDFLoader** to read the text:  \n",
    "   - Extract all text from the PDF.  \n",
    "   - Split the text into smaller parts (chunks) for better processing.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89f4dd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages loaded: 30\n",
      "Total chunks created: 86\n",
      "Pehla chunk example:\n",
      " page_content='Desccribe poijnt -to-point and collective communication in MPI? also discuss how mpi handles \n",
      "communication between nodes in a heterogeneous cluster.  \n",
      "ChatGPT said:  \n",
      "Alright Sami, here‚Äôs your exam -friendly, simple but detailed  explanation:  \n",
      " \n",
      "1. Point -to-Point Communication in MPI  \n",
      "‚Ä¢ Meaning:  One process sends a message directly to another process.  \n",
      "‚Ä¢ Example:  Process 0 sends data to Process 1.  \n",
      "‚Ä¢ Functions Used in MPI:  \n",
      "o MPI_Send()  ‚Üí send a message.' metadata={'producer': 'Microsoft¬Æ Word 2016', 'creator': 'Microsoft¬Æ Word 2016', 'creationdate': '2025-08-10T22:34:31+05:00', 'author': 'Sami', 'moddate': '2025-08-10T22:34:31+05:00', 'source': 'C:\\\\Users\\\\Sami\\\\Desktop\\\\Skills\\\\AI_engineering\\\\2\\\\11_RAG_based_AI_Chatbot\\\\pdc_for_paper.pdf', 'total_pages': 30, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# Step 2 & 3: Load Single PDF and Split into Chunks\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Apni PDF ka direct path do\n",
    "pdf_path = r\"C:\\Users\\Sami\\Desktop\\Skills\\AI_engineering\\2\\11_RAG_based_AI_Chatbot\\pdc_for_paper.pdf\"\n",
    "\n",
    "# PDF load karo\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Total pages loaded: {len(documents)}\")\n",
    "\n",
    "# Text ko chunks me todna\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,    # har chunk ~300 words\n",
    "    chunk_overlap=50   # overlap taake context na tootay\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total chunks created: {len(docs)}\")\n",
    "print(\"Pehla chunk example:\\n\", docs[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a194b2ab",
   "metadata": {},
   "source": [
    "## 3. Create Embeddings and Store in Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95571f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Offline embeddings created and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create Embeddings using SentenceTransformers (Offline)\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Offline model (lightweight and free)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Apne chunks ko embeddings me convert karo aur FAISS me store karo\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Save embeddings locally\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "\n",
    "print(\"‚úÖ Offline embeddings created and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c385246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: What is CUDA?\n",
      "\n",
      "Top 3 relevant chunks:\n",
      "\n",
      "1. Alright Sami, here‚Äôs your simple but detailed  explanation for CUDA  and OpenCL , structured \n",
      "so you can write it easily in tomorrow‚Äôs exam.  \n",
      " \n",
      "1. CUDA (Compute Unified Device Architecture)  \n",
      "Definit...\n",
      "2. work together in parallel computing  ‚Äî this will make your answer more visual and \n",
      "memorable.  \n",
      "Do you want me to add that?  \n",
      "You said:  \n",
      "Cuda and Open CL in detail  \n",
      "ChatGPT said:...\n",
      "3. 4. Summary  \n",
      "‚Ä¢ CUDA  ‚Üí Best performance for NVIDIA GPUs, easy to use, hardware -specific.  \n",
      "‚Ä¢ OpenCL  ‚Üí Portable, works on many devices, but harder to tune for peak performance....\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Store and Test Search in FAISS\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # ya OpenAIEmbeddings agar online use kar rahe ho\n",
    "\n",
    "# Same embeddings model jo tumne create karne ke waqt use kiya tha\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# FAISS index ko load karo (jo tumne step 4 me save kiya tha)\n",
    "vectorstore = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Test query\n",
    "query = \"What is CUDA?\"\n",
    "\n",
    "# Top 3 similar chunks retrieve karo\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(\"üîç Query:\", query)\n",
    "print(\"\\nTop 3 relevant chunks:\\n\")\n",
    "for i, res in enumerate(results, 1):\n",
    "    print(f\"{i}. {res.page_content[:200]}...\")  # sirf pehle 200 characters show karo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd2795",
   "metadata": {},
   "source": [
    "## 4. Retrieve and Generate Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abd4e1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Question: Explain the role of middleware in client / server communcation. Aslo differentiate between 2 -tier and 3 tier client/server architecture.\n",
      "ü§ñ Answer: \n",
      "Answer the question using only the provided context.\n",
      "If you don‚Äôt know, say \"I don‚Äôt know.\"\n",
      "\n",
      "Context: Explain the role of middleware in client / server communcation. Aslo differentiate between 2 -tier \n",
      "and 3 tier client/server architecture.  \n",
      "ChatGPT said:  \n",
      "Alright Sami, here‚Äôs the simple but detailed  explanation you can write in your exam:  \n",
      " \n",
      "1. Role of Middleware in Client/Server Communication  \n",
      "‚Ä¢ Middleware  is software that sits between  the client and the server.  \n",
      "‚Ä¢ It helps them communicate, manage requests, and share data smoothly . \n",
      "‚Ä¢ Main Roles:  \n",
      "1. Communication Management:\n",
      "\n",
      "5. Scalability & Reliability:  \n",
      "‚ñ™ Allows system to handle more clients without breaking.  \n",
      "6. Transparency:  \n",
      "‚ñ™ Client doesn‚Äôt need to know server‚Äôs location or hardware; middleware \n",
      "handles it.  \n",
      "Example:  In a banking app, middleware handles the client‚Äôs request (balance check), talks to the \n",
      "database server, and sends back the result without the client needing to know database details.  \n",
      " \n",
      "2. 2-Tier vs 3 -Tier Client/Server Architecture\n",
      "\n",
      "If you want, I can now combine these into a single neat diagram  showing middleware in 2 -tier \n",
      "and 3 -tier architectures so it‚Äôs easier to remember. This will give your exam answer an edge. Do \n",
      "you want me to make that?  \n",
      "You said:  \n",
      "give in more detail  \n",
      "ChatGPT said:  \n",
      "Alright Sami, let‚Äôs go deeper but still keep it easy to understand  so you can remember and write \n",
      "it well in the exam.  \n",
      " \n",
      "1. Role of Middleware in Client/Server Communication\n",
      "Question: Explain the role of middleware in client / server communcation. Aslo differentiate between 2 -tier and 3 tier client/server architecture.\n",
      "Answer:\n",
      "Middleware plays a crucial role in client-server communication by providing a bridge between the client and server. It facilitates communication, manages requests, and ensures data exchange between the two parties.\n",
      "\n",
      "2-Tier Client/Server Architecture:\n",
      "In a 2-tier architecture, the client is connected to the server through a single network connection. The client is responsible for sending requests to the server, which processes and responds to them. The client and server communicate directly without needing to use multiple layers. This architecture is typically used in applications where the server is not a single entity but a part of a larger system.\n",
      "\n",
      "3-Tier Client/Server Architecture:\n",
      "In a 3-tier architecture, the client is connected to the server through multiple network connections. The client sends requests to the server, which processes them and sends the response. The server may also have its own layer, which handles the data processing and storage. This architecture is commonly used in applications where the server is a central component and can handle multiple clients simultaneously.\n",
      "Answer:\n",
      "The role of middleware in client/server communication is to act as a bridge between the client and server, facilitating communication, managing requests, and ensuring smooth data exchange. \n",
      "\n",
      "2-Tier Client/Server Architecture: \n",
      "In this architecture, the client is directly connected to the server via a\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Retrieval + Generation Pipeline\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEndpoint   # Offline ke liye tum HF model bhi use kar sakte ho\n",
    "\n",
    "# 1. Embeddings (same model jo tumne pehle use kiya tha)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. FAISS se load karo (jisme tumne step 5 me save kiya tha)\n",
    "vectorstore = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# 3. Retriever banao\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# 4. Prompt template\n",
    "template = \"\"\"\n",
    "Answer the question using only the provided context.\n",
    "If you don‚Äôt know, say \"I don‚Äôt know.\"\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# 5. LLM (Offline HuggingFace model)\n",
    "from transformers import pipeline\n",
    "local_llm = pipeline(\"text-generation\", model=\"Qwen/Qwen3-0.6B\")  # tum koi bhi HF model use kar sakte ho\n",
    "\n",
    "# Wrapper (LangChain friendly)\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=local_llm)\n",
    "\n",
    "# 6. RetrievalQA Chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",   # context ko \"stuff\" karke LLM ko bhejna\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# 7. Test Query\n",
    "query = \"Explain the role of middleware in client / server communcation. Aslo differentiate between 2 -tier and 3 tier client/server architecture.\"\n",
    "result = qa.run(query)\n",
    "\n",
    "print(\"üîç Question:\", query)\n",
    "print(\"ü§ñ Answer:\", result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
